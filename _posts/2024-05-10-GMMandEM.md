---
title: EM算法和高斯混合模型(GMM)
date: 2024-05-10 23:00:00 +0800
categories: [机器学习]
tags: [机器学习, 优化算法]
toc: true 
comments: false
math: true

---

# EM算法

EM算法是一种**迭代算法**，用于**含隐变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计**。EM算法的每次迭代由两步组成：**E 步，求期望（expectation）**; **M 步，求极大（maximization）**。所以这一算法称为**期望极大算法**（expectation maximization algorithm），简称 EM 算法。

## 预备知识

### Jensen不等式

当$f(x)$为凸函数时，有

$$
f(E(x))≤E(f(x))
$$

当$f(x)$为上凸函数(形如对数函数)时，则相反

$$
f(E(x))≥E(f(x))
$$

### 观测变量和隐变量

隐变量(latent variable)，顾名思义，就是通过观测不能直接得出的变量。

例如**三硬币模型**：假设有3枚硬币，分别记作$A,B,C$。这些硬币正面出现的概率分别是$\pi,p,q$。进行如下掷硬币实验：先掷硬币$A$，根据其结果选出硬币$B$或硬币$C$，正面选硬币$B$，反面选硬币$C$；然后掷选出的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复n次试验。

其中，结果$B$或$C$的正或反面都是可以观察得到的，这就是**观测变量**；而投掷硬币$A$的结果通过观测不能得出，这就是**隐变量**。本文用$y$表示观测变量，用$z$表示隐变量，$\theta$ 表示模型参数。

模型参数$\theta$ 的极大似然估计为，

$$
\hat{\theta}=\underset{\theta}{\operatorname{argmax}}logP(Y|\theta)
$$

其中，观测数据的似然函数$P(Y|\theta)=P(Y|Z,\theta)P(Z|\theta)$。此问题无解析解，只能通过迭代的方法求出。

## 算法原理
