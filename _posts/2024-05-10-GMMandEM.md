---
title: EM算法和高斯混合模型(GMM)
date: 2024-04-21 22:00:00 +0800
categories: [机器学习]
tags: [机器学习, 优化算法]
toc: true 
comments: false
math: true

---

# EM算法

EM算法是一种**迭代算法**，用于**含隐变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计**。EM算法的每次迭代由两步组成：**E 步，求期望（expectation）**; **M 步，求极大（maximization）**。所以这一算法称为**期望极大算法**（expectation maximization algorithm），简称 EM 算法。

## 预备知识

### Jensen不等式

当$f(x)$为凸函数时，有

$$
f(E(x))≤E(f(x))
$$

当$f(x)$为上凸函数(形如对数函数)时，则相反

$$
f(E(x))≥E(f(x))
$$

### 观测变量和隐变量

隐变量(latent variable)，顾名思义，就是通过观测不能直接得出的变量。

例如**三硬币模型**：假设有3枚硬币，分别记作$A,B,C$。这些硬币正面出现的概率分别是$\pi,p,q$。进行如下掷硬币实验：先掷硬币$A$，根据其结果选出硬币$B$或硬币$C$，正面选硬币$B$，反面选硬币$C$；然后掷选出的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复n次试验。

其中，结果$B$或$C$的正或反面都是可以观察得到的，这就是**观测变量**；而投掷硬币$A$的结果通过观测不能得出，这就是**隐变量**。本文用$x$表示观测变量，用$z$表示隐变量，$\theta$ 表示模型参数。

模型参数$\theta$ 的极大似然估计为，

$$
\hat{\theta}=\underset{\theta}{\operatorname{argmax}}\sum_{i=1}^nlogP(x_i|\theta) 
$$

如若含有隐含变量$z=(z_1,z_2,...,z_n)$ 则上式变为：

$$
\hat{\theta}=\underset{\theta}{\operatorname{argmax}}\sum_{i=1}^nlog\sum_{z_i}P(x_i,z_i|\theta) 
$$

上式表示求一个$\hat{\theta}$ 让已知观测$Y$ 出现的概率最大。当出现隐含变量时，此问题无解析解，只能通过迭代的方法求出。

## 算法原理

首先，引入$z_i$的概率密度分布$Q_i(z_i)$，引入Jensen不等式，则函数变为：

$$
\begin{aligned}
L(\theta)=\sum_{i=1}^{n} \log \sum_{z_{i}} p\left(x_{i}, z_{i} ; \theta\right) & =\sum_{i=1}^{n} \log \sum_{z_{i}} Q_{i}\left(z_{i}\right) \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)} \\
& \geq \sum_{i=1}^{n} \sum_{z_{i}} Q_{i}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}
\end{aligned}
$$

其中的期望关系可以回顾Lazy Statistician规则(Actually I also don't understand，whatever...)。

等号成立需要满足$\frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}=c $。由于$Q_i(z_i)$是一个分布，满足$\sum_z Q_i(z_i)=1$，因此，$\sum p(x_i,z_i;\theta)=c$。进一步得到，$Q_i(z_i)=\frac{p(x_i,z_i;\theta)}{\sum_z p(x_i,z_i;\theta)}=\frac{p(x_i,z_i;\theta)}{p(x_i;\theta)}=p(z_i|x_i;\theta_i)。$

### 算法步骤：

begin:随机初始化$\theta$初值

$j=1,2,...,J$开始迭代

E step:

    $Q_i(z_i)=p(z_i|x_i;\theta_i)$

    $L(\theta,\theta_j)= \sum_{i=1}^{n} \sum_{z_{i}} Q_{i}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}
$

M step:

    $\theta_{j+1}=argmaxL(\theta,\theta_{j})$

当$\theta_{j+1}$ 收敛，end！

### 收敛性证明

略

[参考文章](https://eipi10.cn/algorithm/2020/07/24/em_1/)

# 高斯混合模型(GMM)

## GMM简介

    高斯混合模型（Gaussian Mixture Model，GMM）是单一高斯概率密度函数的延伸，就是用多个高斯概率密度函数（正态分布曲线）精确地量化变量分布，是将变量分布分解为K个基于高斯概率密度函数（正态分布曲线）分布的统计模型。GMM是一种常用的聚类算法，一般使用期望最大算法（Expectation Maximization，EM）进行估计。

    对于$n$维样本空间中的随机向量$x$，服从高斯分布$x\sim N(\mu,\Sigma)$，其概率密度函数如下：

$$
p(x)=\frac{1}{(2 \pi)^{\frac{2}{n}} \Sigma^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \sum^{-1}(x-\mu)}{\tiny } 
$$

    GMM的基本假设为数据是由几个不同的高斯分布的随机变量组合而成。在高斯混合模型中，我们需要估计每一个高斯分布的均值与方差。从最大似然估计的角度来说，给定某个有$n$个样本的数据集$X$，假如已知GMM中一共有$k$簇，我们就是要找到$k$组均值$\mu_1,...,\mu_k$，$k$组方差$\sigma_1,...,\sigma_k$来最大化似然函数$L$。然而，直接计算似然函数比较复杂，因此需要引入隐变量，即每个样本属于每一簇的概率$W$。

## EM求解GMM

每次迭代的目标函数为：

$$
Q\left(\Theta, \Theta^{t}\right)=\sum_{i} \sum_{k} \omega_{i, k}^{t} \ln \frac{\alpha_{k}}{\omega_{i, k}^{t} \sqrt{(2 \pi)^{d} \operatorname{det}\left(\Sigma_{k}\right)}} \exp \left[-\frac{1}{2}\left(x_{i}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{i}-\mu_{k}\right)\right]
$$

其中$\omega_{i, k}^{t}$表示第$t$次迭代中，第$i$个样本属于第$k$簇的概率；$\alpha_k$表示第$k$簇的比重。

**E Step：**

更新样本属于簇的概率$\omega_{i, k}^{t}$：

$$
\omega_{i, k}^{t}=\frac{\alpha_{k}^{t} N\left(x_{i} \mid \mu_{k}^{t}, \Sigma_{k}^{t}\right)}{\sum_{k} \alpha_{k}^{t} N\left(x_{i} \mid \mu_{k}^{t}, \Sigma_{k}^{t}\right)}
$$

更新后，目标函数更新如下：

$$
\begin{array}{l}
Q\left(\Theta, \Theta^{t}\right) =\\
\sum_{i} \sum_{k} \omega_{i, k}^{t}\left(\ln \alpha_{k}-\ln \omega_{i, k}^{t}-\frac{d}{2} \ln \sqrt{(2 \pi)^{d}}-\frac{1}{2} \ln \operatorname{det}\left(\Sigma_{k}\right)-\frac{1}{2}\left(x_{i}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{i}-\mu_{k}\right)\right)
\end{array}
$$

**M Step:**

更新$\mu_k$：

$$
\mu_{k}^{t+1}  =\frac{\sum_{i} \omega_{i, k}^{t} x_{i}}{\sum_{i} \omega_{i, k}^{t}}
$$

更新$\Sigma_k$：

$$
\Sigma_{k}^{t+1}  =\frac{\sum_{i} \omega_{i, k}^{t}\left(x_{i}-\mu_{k}^{t+1}\right)\left(x_{i}-\mu_{k}^{t+1}\right)^{T}}{\sum_{i} \omega_{i, k}^{t}}
$$

更新$\alpha_k$：

$$
\alpha_{k}^{t+1}=\frac{\sum_i\omega_{i,k}^t}{N}
$$

**总结：**

![](\assets\img\GMMandEM1.png)

## 代码实现

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal


# 初始化参数
# MU1    = [1 2];
# SIGMA1 = [1 0; 0 0.5];
# MU2    = [-1 -1];
# SIGMA2 = [1 0; 0 1];
# 
# 1与2各生成1000个散点，共2000个散点。
MU1 = np.array([1, 2])
SIGMA1 = np.array([[1, 0], [0, 0.5]])
MU2 = np.array([-1, -1])
SIGMA2 = np.array([[1, 0], [0, 1]])

# 生成数据点
data1 = np.random.multivariate_normal(MU1, SIGMA1, 1000)
data2 = np.random.multivariate_normal(MU2, SIGMA2, 1000)

X = np.concatenate([data1, data2])

# 打乱X
np.random.shuffle(X)
# 显示散点分布
plt.scatter(data1[:, 0], data1[:, 1], label='Class 1')
plt.scatter(data2[:, 0], data2[:, 1], label='Class 2')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Scatter Plot of Data Points')
plt.legend()
plt.show()


def gaussian_2d(X, mu, sigma):
    x, y = X
    mu_x, mu_y = mu
    sigma_x = sigma[0][0]
    sigma_y = sigma[1][1]
    cov_xy = sigma[0][1]
    
    # 计算协方差矩阵的行列式
    det_sigma = sigma_x * sigma_y - cov_xy ** 2
    
    # 如果行列式为零，则返回一个很小的值，以避免除以零错误
    if det_sigma == 0:
        det_sigma = 1e-10
    
    # 计算协方差矩阵的逆
    sigma_inv = np.linalg.inv(sigma)
    
    # 计算指数部分
    exponent = -0.5 * (np.dot(np.dot([x - mu_x, y - mu_y], sigma_inv), [x - mu_x, y - mu_y]))
    
    # 计算系数
    coefficient = 1 / (2 * np.pi * np.sqrt(det_sigma))
    
    return coefficient * np.exp(exponent)

def my_fit_GMM(X, maxIter=1000):
    X = np.array(X, dtype=float)
    Xi = X.shape[0]
    K = 2
    
    # 初始化均值、方差、权重系数
    means = np.array([[1, 1], [-1, -1]], dtype=float)
    cov = np.array([[[0.1, 0], [0, 0.1]], [[0.1, 0], [0, 1]]], dtype=float)   
    weight = np.array([0.5, 0.5])
    # W即为课件中的E
    W = np.zeros(shape=(Xi, K), dtype=float)
   
    for iter in range(maxIter):
        old_means = np.copy(means)
        W_sum = np.zeros(K)
        W_sum_X = np.zeros((K, X.shape[1]))
        W_sum_cov = np.zeros((K, X.shape[1], X.shape[1]))
        
        # E Step
        for i in range(Xi):
            for k in range(K):
                W[i, k] = weight[k] * gaussian_2d(X[i], means[k], cov[k])
            W[i] /= np.sum(W[i])
        
        # M Step
        for k in range(K):
            for i in range(Xi):
                W_sum[k] += W[i, k]
                W_sum_X[k] += W[i, k] * X[i]
                W_sum_cov[k] += W[i, k] * np.outer(X[i] - means[k], X[i] - means[k])
            means[k] = np.divide(W_sum_X[k], W_sum[k], out=np.zeros_like(W_sum_X[k]), where=W_sum[k]!=0)
            cov[k] = np.divide(W_sum_cov[k], W_sum[k], out=np.zeros_like(W_sum_cov[k]), where=W_sum[k]!=0)
            weight[k] = W_sum[k] / Xi

        if np.allclose(means, old_means, atol=1e-8):
            break

    return means, cov, weight, iter

def draw_results(m,s,w):

    # 定义两个二维高斯分布的参数
    m1 = m[0]
    s1 = s[0]

    m2 = m[1]
    s2 = s[1]

    # 生成网格点
    x, y = np.mgrid[-5:5:.01, -5:5:.01]
    pos = np.dstack((x, y))

    # 计算每个点的概率密度值
    rv1 = multivariate_normal(m1, s1)
    rv2 = multivariate_normal(m2, s2)
    z1 = rv1.pdf(pos)
    z2 = rv2.pdf(pos)

    # 混合两个高斯分布的概率密度函数
    z = w[0] * z1 + w[1] * z2  # 设置混合比例

    # 绘制3D图
    fig = plt.figure(figsize=(5, 10))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(x, y, z, cmap='viridis')

    # 设置坐标轴标签和标题
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Probability Density')
    ax.set_title('3D Plot of Mixture Gaussian Probability Density')

    plt.show()

means, cov, weights, iter = my_fit_GMM(X)
draw_results(means, cov, weights)
print("Data1.Means:")
print(means[0])
print("Data2.Means:")
print(means[1])

print("Data1.Covariances:")
print(cov[0])
print("Data2.Covariances:")
print(cov[1])

print("Weights:")
print(weights)
print("iter = ", iter)


```
