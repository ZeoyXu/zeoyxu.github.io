---
title: EM算法和高斯混合模型(GMM)
date: 2024-05-10 23:00:00 +0800
categories: [机器学习]
tags: [机器学习, 优化算法]
toc: true 
comments: false
math: true

---

# EM算法

EM算法是一种**迭代算法**，用于**含隐变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计**。EM算法的每次迭代由两步组成：**E 步，求期望（expectation）**; **M 步，求极大（maximization）**。所以这一算法称为**期望极大算法**（expectation maximization algorithm），简称 EM 算法。

## 预备知识

### Jensen不等式

当$f(x)$为凸函数时，有

$$
f(E(x))≤E(f(x))
$$

当$f(x)$为上凸函数(形如对数函数)时，则相反

$$
f(E(x))≥E(f(x))
$$

### 观测变量和隐变量

隐变量(latent variable)，顾名思义，就是通过观测不能直接得出的变量。

例如**三硬币模型**：假设有3枚硬币，分别记作$A,B,C$。这些硬币正面出现的概率分别是$\pi,p,q$。进行如下掷硬币实验：先掷硬币$A$，根据其结果选出硬币$B$或硬币$C$，正面选硬币$B$，反面选硬币$C$；然后掷选出的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复n次试验。

其中，结果$B$或$C$的正或反面都是可以观察得到的，这就是**观测变量**；而投掷硬币$A$的结果通过观测不能得出，这就是**隐变量**。本文用$x$表示观测变量，用$z$表示隐变量，$\theta$ 表示模型参数。

模型参数$\theta$ 的极大似然估计为，

$$
\hat{\theta}=\underset{\theta}{\operatorname{argmax}}\sum_{i=1}^nlogP(x_i|\theta) 
$$

如若含有隐含变量$z=(z_1,z_2,...,z_n)$ 则上式变为：

$$
\hat{\theta}=\underset{\theta}{\operatorname{argmax}}\sum_{i=1}^nlog\sum_{z_i}P(x_i,z_i|\theta) 
$$

上式表示求一个$\hat{\theta}$ 让已知观测$Y$ 出现的概率最大。当出现隐含变量时，此问题无解析解，只能通过迭代的方法求出。

## 算法原理

首先，引入$z_i$的概率密度分布$Q_i(z_i)$，引入Jensen不等式，则函数变为：

$$
\begin{aligned}
L(\theta)=\sum_{i=1}^{n} \log \sum_{z_{i}} p\left(x_{i}, z_{i} ; \theta\right) & =\sum_{i=1}^{n} \log \sum_{z_{i}} Q_{i}\left(z_{i}\right) \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)} \\
& \geq \sum_{i=1}^{n} \sum_{z_{i}} Q_{i}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}
\end{aligned}
$$

其中的期望关系可以回顾Lazy Statistician规则(Actually I also don't understand，whatever...)。

等号成立需要满足$\frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}=c $。由于$Q_i(z_i)$是一个分布，满足$\sum_z Q_i(z_i)=1$，因此，$\sum p(x_i,z_i;\theta)=c$。进一步得到，$Q_i(z_i)=\frac{p(x_i,z_i;\theta)}{\sum_z p(x_i,z_i;\theta)}=\frac{p(x_i,z_i;\theta)}{p(x_i;\theta)}=p(z_i|x_i;\theta_i)。$

### 算法步骤：

begin:随机初始化$\theta$初值

$j=1,2,...,J$开始迭代

E step:

    $Q_i(z_i)=p(z_i|x_i;\theta_i)$

    $L(\theta,\theta_j)= \sum_{i=1}^{n} \sum_{z_{i}} Q_{i}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}
$

M step:

    $\theta_{j+1}=argmaxL(\theta,\theta_{j})$

当$\theta_{j+1}$ 收敛，end！

### 收敛性证明

略

[参考文章](https://eipi10.cn/algorithm/2020/07/24/em_1/)

# 高斯混合模型(GMM)

## GMM简介

    高斯混合模型（Gaussian Mixture Model，GMM）是单一高斯概率密度函数的延伸，就是用多个高斯概率密度函数（正态分布曲线）精确地量化变量分布，是将变量分布分解为K个基于高斯概率密度函数（正态分布曲线）分布的统计模型。GMM是一种常用的聚类算法，一般使用期望最大算法（Expectation Maximization，EM）进行估计。

    对于$n$维样本空间中的随机向量$x$，服从高斯分布$x\sim N(\mu,\Sigma)$，其概率密度函数如下：

$$
p(x)=\frac{1}{(2 \pi)^{\frac{2}{n}} \Sigma^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \sum^{-1}(x-\mu)}{\tiny } 
$$

    GMM的基本假设为数据是由几个不同的高斯分布的随机变量组合而成。在高斯混合模型中，我们需要估计每一个高斯分布的均值与方差。从最大似然估计的角度来说，给定某个有$n$个样本的数据集$X$，假如已知GMM中一共有$k$簇，我们就是要找到$k$组均值$\mu_1,...,\mu_k$，$k$组方差$\sigma_1,...,\sigma_k$来最大化似然函数$L$。然而，直接计算似然函数比较复杂，因此需要引入隐变量，即每个样本属于每一簇的概率$W$。

## EM求解GMM

每次迭代的目标函数为：

$$
Q\left(\Theta, \Theta^{t}\right)=\sum_{i} \sum_{k} \omega_{i, k}^{t} \ln \frac{\alpha_{k}}{\omega_{i, k}^{t} \sqrt{(2 \pi)^{d} \operatorname{det}\left(\Sigma_{k}\right)}} \exp \left[-\frac{1}{2}\left(x_{i}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{i}-\mu_{k}\right)\right]
$$

其中$\omega_{i, k}^{t}$表示第$t$次迭代中，第$i$个样本属于第$k$簇的概率；$\alpha_k$表示第$k$簇的比重。

**E Step：**

更新样本属于簇的概率$\omega_{i, k}^{t}$：

$$
\omega_{i, k}^{t}=\frac{\alpha_{k}^{t} N\left(x_{i} \mid \mu_{k}^{t}, \Sigma_{k}^{t}\right)}{\sum_{k} \alpha_{k}^{t} N\left(x_{i} \mid \mu_{k}^{t}, \Sigma_{k}^{t}\right)}
$$

更新$\alpha_k$：

$$
\alpha_{k}^{t+1}=\frac{\sum_i\omega_{i,k}^t}{N}
$$

更新后，目标函数更新如下：

$$
\begin{array}{l}
Q\left(\Theta, \Theta^{t}\right) =\\
\sum_{i} \sum_{k} \omega_{i, k}^{t}\left(\ln \alpha_{k}-\ln \omega_{i, k}^{t}-\frac{d}{2} \ln \sqrt{(2 \pi)^{d}}-\frac{1}{2} \ln \operatorname{det}\left(\Sigma_{k}\right)-\frac{1}{2}\left(x_{i}-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_{i}-\mu_{k}\right)\right)
\end{array}
$$

**M Step:**

更新$\mu_k$：

$$
\mu_{k}^{t+1}  =\frac{\sum_{i} \omega_{i, k}^{t} x_{i}}{\sum_{i} \omega_{i, k}^{t}}
$$

更新$\Sigma_k$：

$$
\Sigma_{k}^{t+1}  =\frac{\sum_{i} \omega_{i, k}^{t}\left(x_{i}-\mu_{k}^{t+1}\right)\left(x_{i}-\mu_{k}^{t+1}\right)^{T}}{\sum_{i} \omega_{i, k}^{t}}
$$

**总结：**

![](\assets\img\GMMandEM1.png)

## 代码实现

```python

```
