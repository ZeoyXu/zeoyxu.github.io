---
title: EM算法和高斯混合模型(GMM)
date: 2024-05-10 23:00:00 +0800
categories: [机器学习]
tags: [机器学习, 优化算法]
toc: true 
comments: false
math: true

---

# EM算法

EM算法是一种**迭代算法**，用于**含隐变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计**。EM算法的每次迭代由两步组成：**E 步，求期望（expectation）**; **M 步，求极大（maximization）**。所以这一算法称为**期望极大算法**（expectation maximization algorithm），简称 EM 算法。

## 预备知识

### Jensen不等式

当$f(x)$为凸函数时，有

$$
f(E(x))≤E(f(x))
$$

当$f(x)$为上凸函数(形如对数函数)时，则相反

$$
f(E(x))≥E(f(x))
$$

### 观测变量和隐变量

隐变量(latent variable)，顾名思义，就是通过观测不能直接得出的变量。

例如**三硬币模型**：假设有3枚硬币，分别记作$A,B,C$。这些硬币正面出现的概率分别是$\pi,p,q$。进行如下掷硬币实验：先掷硬币$A$，根据其结果选出硬币$B$或硬币$C$，正面选硬币$B$，反面选硬币$C$；然后掷选出的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复n次试验。

其中，结果$B$或$C$的正或反面都是可以观察得到的，这就是**观测变量**；而投掷硬币$A$的结果通过观测不能得出，这就是**隐变量**。本文用$x$表示观测变量，用$z$表示隐变量，$\theta$ 表示模型参数。

模型参数$\theta$ 的极大似然估计为，

$$
\hat{\theta}=\underset{\theta}{\operatorname{argmax}}\sum_{i=1}^nlogP(x_i|\theta) 
$$

如若含有隐含变量$z=(z_1,z_2,...,z_n)$ 则上式变为：

$$
\hat{\theta}=\underset{\theta}{\operatorname{argmax}}\sum_{i=1}^nlog\sum_{z_i}P(x_i,z_i|\theta) 
$$



上式表示求一个$\hat{\theta}$ 让已知观测$Y$ 出现的概率最大。当出现隐含变量时，此问题无解析解，只能通过迭代的方法求出。

## 算法原理

首先，引入$z_i$的概率密度分布$Q_i(z_i)$，引入Jensen不等式，则函数变为：

$$
\begin{aligned}
L(\theta)=\sum_{i=1}^{n} \log \sum_{z_{i}} p\left(x_{i}, z_{i} ; \theta\right) & =\sum_{i=1}^{n} \log \sum_{z_{i}} Q_{i}\left(z_{i}\right) \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)} \\
& \geq \sum_{i=1}^{n} \sum_{z_{i}} Q_{i}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}
\end{aligned}
$$

其中的期望关系可以回顾Lazy Statistician规则(Actually I also don't understand，whatever...)。

等号成立需要满足$\frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}=c $。由于$Q_i(z_i)$是一个分布，满足$\sum_z Q_i(z_i)=1$，因此，$\sum p(x_i,z_i;\theta)=c$。进一步得到，$Q_i(z_i)=\frac{p(x_i,z_i;\theta)}{\sum_z p(x_i,z_i;\theta)}=\frac{p(x_i,z_i;\theta)}{p(x_i;\theta)}=p(z_i|x_i;\theta_i)。$

**算法步骤：**

begin:随机初始化$\theta$初值

$j=1,2,...,J$开始迭代

E step:

    $Q_i(z_i)=p(z_i|x_i;\theta_i)$

    $L(\theta,\theta_j)= \sum_{i=1}^{n} \sum_{z_{i}} Q_{i}\left(z_{i}\right) \log \frac{p\left(x_{i}, z_{i} ; \theta\right)}{Q_{i}\left(z_{i}\right)}
$

M step:

    $\theta_{j+1}=argmaxL(\theta,\theta_{j})$

当$\theta_{j+1}$ 收敛，end！

收敛性证明，略

[参考文章](https://eipi10.cn/algorithm/2020/07/24/em_1/)





















    
